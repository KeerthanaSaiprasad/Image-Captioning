{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K0tdHeHBRRE"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BErDBLZtBRRJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEgelk8aLyeu",
    "outputId": "4378fa54-2610-435a-be6f-ef39e23377b9"
   },
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0-IOpYLL6_d",
    "outputId": "3698439b-0936-4789-e0c6-9c042a2ab155"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y1wR6fDMAOX"
   },
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHpcway4MN1f"
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arugWsHDMVG5",
    "outputId": "6a78590e-c26b-4a66-a8d2-b558ef42ad66"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download adityajn105/flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh7W74qOMgwQ"
   },
   "outputs": [],
   "source": [
    "! unzip flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfG8mvTHDAZg"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/kaggle/input/flickr8k'\n",
    "WORKING_DIR = '/content/working'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Jj6JtKBRRL"
   },
   "source": [
    "## Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjZUiqwSBRRM",
    "outputId": "689e0520-3a7c-4b8c-b584-32bee8a0ff0b"
   },
   "outputs": [],
   "source": [
    "# load vgg16 model\n",
    "model = VGG16()\n",
    "# restructure the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "# summarise\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LpYC10ON1Gw",
    "outputId": "346a1299-0cf2-47d6-b375-f14f9e035584"
   },
   "outputs": [],
   "source": [
    "print(os.listdir('/content/Images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "db8108ce66334c5aa792229beddae7b7"
     ]
    },
    "id": "hhHp380OBRRM",
    "outputId": "6defab95-e2a2-4e92-951e-206bc3ac3313"
   },
   "outputs": [],
   "source": [
    "# extract features from images\n",
    "\n",
    "features = {}\n",
    "directory = '/content/Images'\n",
    "\n",
    "for img_name in tqdm(os.listdir('/content/Images')):\n",
    "    # load the image from file\n",
    "    img_path = directory + '/' + img_name\n",
    "    image = load_img(img_path, target_size = (224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose = 0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGfMKX8OqLy4"
   },
   "outputs": [],
   "source": [
    "with open('/content/features.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W4MmyepBRRO"
   },
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T10nlZT5qfpZ"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('features.pkl'), 'rb') as f: features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJzbUWVrBRRP"
   },
   "source": [
    "## Load the Captions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJb-2Um4BRRP"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "16a68a6887b34f7e9ebd9989e7a32614",
      "bcd16f73943e4812b7d792ffb71db12f",
      "6557b9b4a8004375baa133d1e1fb8935",
      "9ff3c3a0b3fd455fb306612303ff1481",
      "62cfbb81f8bd419a9be906156f9d6852",
      "c2f157e347104224aae1ed06908afb28",
      "7e480fe60d2a40f1be394a1c32499aed",
      "2875082b2059485e9dbf46f505a74a31",
      "09b2b1ad509344ce9d02fc590a80773f",
      "6afffd43a75e462a852f66e800a69b93",
      "9af1daff6fe84a728e0e3d233538277c"
     ]
    },
    "id": "MKZqUdPCBRRQ",
    "outputId": "195238f4-50ac-4004-c772-b1b5adaf9355"
   },
   "outputs": [],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the lines by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K85shSTLBRRQ"
   },
   "source": [
    "## Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd1jz2mLBRRR"
   },
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lower case\n",
    "            caption = caption.lower()\n",
    "            # delete special characters, digits, etc.\n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdoRABbtBRRR",
    "outputId": "a3bc110b-a7b7-4276-887f-fc119baf407a"
   },
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mof4jH_YBRRS"
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikaFRLKIBRRS",
    "outputId": "1c15ce09-0514-42ab-a2b6-d0d9824bd43b"
   },
   "outputs": [],
   "source": [
    "# after preprocessing of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkz6yybnBRRS"
   },
   "outputs": [],
   "source": [
    "all_caption = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_caption.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tO7GIeFlBRRT",
    "outputId": "751c25f3-d013-457b-86c3-ba7e326f7b8a"
   },
   "outputs": [],
   "source": [
    "len(all_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYokr9YXBRRT",
    "outputId": "c3605058-c0be-4fed-8b3f-914f6ff886c0"
   },
   "outputs": [],
   "source": [
    "all_caption[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB0j8dRvBRRT"
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_caption)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KALPROMCBRRU",
    "outputId": "66c7fdc4-cbb9-4760-cb71-d012e01a54ea"
   },
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLiN3AU_BRRU",
    "outputId": "6d17c6e7-fabd-479b-baab-7a1672b05e69"
   },
   "outputs": [],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_caption)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BkhU498BRRU"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ikY2qGeBRRU"
   },
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G6zFnPFBRRU"
   },
   "outputs": [],
   "source": [
    "### This is how the whole sequence splitting works\n",
    "\n",
    "### startseq girl going into wooden building endseq\n",
    "### X                                          y\n",
    "# startseq                                    girl\n",
    "# startseq girl                               going\n",
    "# startseq girl going                         into\n",
    "# ...\n",
    "# startseq girl going into wooden building    endseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRFYFvQ7BRRV"
   },
   "outputs": [],
   "source": [
    "# create data generator to get data in batch (avoids session crash)\n",
    "\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # store the sequence\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxVSqzh_BRRV"
   },
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "vuJ3PhowBRRV",
    "outputId": "efebf212-58d0-4e66-b9b5-4114def91764"
   },
   "outputs": [],
   "source": [
    "# ENCODER MODEL\n",
    "# image feature layers\n",
    "input1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(input1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# sequence feature layers\n",
    "input2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(input2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# DECODER MODEL\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjH_qrMLBRRV",
    "outputId": "0d707374-d609-44fa-9d78-a1e7a287fafa"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs = 1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIdd-k42BRRV"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(WORKING_DIR + '/model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aYVXT5bBRRW"
   },
   "source": [
    "## Generate Captions for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOkGdxbXBRRW"
   },
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjFfeKFqBRRY"
   },
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "fec6900990344b6d81f23e26ec50d431",
      "0059f3ed929f4a729271497349bc5e73",
      "112762444f29438d801be0e5ba81f44a",
      "6df9ce2105064934badd5b8fdc8aecc3",
      "10cc252f4974420aab79e92778b170a7",
      "bb7f6ab88134475bb943da5c118e9bb4",
      "48fa920ad88740f081b2a8a6da2ba26a",
      "84d681b40c554d91a642af6ce8962b7a",
      "6ed6f2eb53644c088dad088ca803d624",
      "83d97aa785ad4249bc27aaff50fb557a",
      "ed9e04a5e99148f281145464fd69ab28"
     ]
    },
    "id": "j_-js5nBBRRY",
    "outputId": "9c3cc6ef-5da2-4a13-efe8-612ebb33576a"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_caption = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_caption)\n",
    "    predicted.append(y_pred)\n",
    "\n",
    "# calculate the BLEU score\n",
    "print('BLEU-1: %f' %corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' %corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ZjUmpMBRRZ"
   },
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrMKf6KYBRRZ"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# load the image\n",
    "def generate_caption(image_name):\n",
    "    image_name = \"1001773457_577c3a7d70.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path1 = '/content/Images/1001773457_577c3a7d70.jpg'\n",
    "    #img_path2 =\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('------------------------Actual------------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    print('\\n')\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('------------------------Predicted------------------------')\n",
    "    print(y_pred)\n",
    "    print('\\n')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "hMY-OjuhBRRZ",
    "outputId": "39b55217-a4ca-410f-b113-5475a2be19b0"
   },
   "outputs": [],
   "source": [
    "generate_caption('/content/Images/1001773457_577c3a7d70.jpg')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
